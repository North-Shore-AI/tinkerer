experiment_name: claim-extractor-scifact

data:
  train_path: data/processed/scifact_claim_extractor_clean.jsonl
  eval_path: data/raw/scifact/claims_dev.jsonl
  max_samples: null
  shuffle: true
  batch_size: 8

model:
  base_model: "meta-llama/Llama-3.1-8B-Instruct"
  adapter_name: "claim-extractor-scifact"
  lora:
    r: 16
    alpha: 32
    dropout: 0.05

optimization:
  learning_rate: 2.0e-4  # Increased from 1.5e-4 per hyperparameter sweep recommendation
  weight_decay: 0.0
  gradient_clip: 1.0
  epochs: 5  # Increased from 3 to allow more learning

training:
  # Citation hallucination fix (2025-11-18)
  # See: docs/20251118/antagonist-mvp-review/HIGH_SEVERITY_REVIEW.md
  cns_claim_evidence_weight: 3.0  # Heavily weight evidence grounding
  citation_validity_weight: 2.0   # Penalize citing non-existent documents
  validate_citations_during_training: true  # Enable citation validation in training loop

logging:
  eval_every_steps: 200
  save_every_steps: 1000
  wandb_project: null
  notes: "SciFact training config with citation hallucination fixes (2025-11-18)."
