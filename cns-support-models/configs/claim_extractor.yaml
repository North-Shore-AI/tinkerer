# Base configuration for the claim & hypothesis extractor LoRA training loop.
experiment_name: claim-extractor-demo

data:
  train_path: data/samples/claim_extractor.jsonl
  eval_path: null
  max_samples: null        # set to an integer to subsample
  shuffle: true
  batch_size: 4

model:
  base_model: "meta-llama/Llama-3.1-8B-Instruct"
  adapter_name: "claim-extractor-lora"
  lora:
    r: 16
    alpha: 32
    dropout: 0.05

optimization:
  learning_rate: 1.0e-4
  weight_decay: 0.0
  gradient_clip: 1.0
  epochs: 3

logging:
  eval_every_steps: 100
  save_every_steps: 500
  wandb_project: null
  notes: "Demo config â€“ tune for real datasets."
